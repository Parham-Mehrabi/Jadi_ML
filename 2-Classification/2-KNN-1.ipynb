{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘img.png’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "!wget -nc -O img.png \"https://camo.githubusercontent.com/af5b58e7980908cb5b46b6f7bd6ca48cd8eab026e31d942a199cfbaaba833637/68747470733a2f2f63662d636f75727365732d646174612e73332e75732e636c6f75642d6f626a6563742d73746f726167652e617070646f6d61696e2e636c6f75642f49424d446576656c6f706572536b696c6c734e6574776f726b2d4d4c30313031454e2d536b696c6c734e6574776f726b2f6c6162732f4d6f64756c65253230332f696d616765732f4b4e4e5f4469616772616d2e706e67\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor(KNN)\n",
    "\n",
    "- in this method we look at K nearest neighbors around the target and select its label based on them\n",
    "- How to choose K (low -> noise & over-fit; high -> too general). Use different Ks with test and check which one is better\n",
    "- KNN can also be used to compute a continuous target(regression)\n",
    "\n",
    "<img src='./img.png'>\n",
    "\n",
    "## evaluation methods:\n",
    " - Jaccard index\n",
    " -  F1-score\n",
    " - Log Loss\n",
    " <hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Index: \n",
    "$ y $ : Actual Labels  \n",
    "$ \\hat{y} $ : Predicted Labels  \n",
    "$ J (y , \\hat{y}) = \\frac{| y \\cap  \\hat{y}| }{ | y \\cup \\hat{y} | } = \\frac { | y \\cap  \\hat{y} | } {|y| + |\\hat{y}| - | y \\cap  \\hat{y}|}  $\n",
    "\n",
    "$ y $ : [0, 0, 0, 1, 1, 0, 1, 1, 0, 1]  \n",
    "$ \\hat{y} $ : [1, 1, 0, 1, 1, 0, 1, 1, 0, 1]  \n",
    "  \n",
    "there is a total of 10 labels where we predict 8 of them correctly:  \n",
    "$ j(y, \\hat{y}) = \\frac {8} {10 + 10 - 8}  = 0.66 $  \n",
    "\n",
    "sklearn.metrics.jaccard_score, will calculate the value for each label and it can return the average:\n",
    "the average method can be change as this:\n",
    "- None, the scores for each class are returned.\n",
    "\n",
    "- 'binary':\n",
    "Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "\n",
    "- 'micro':\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "- 'macro':\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "- 'weighted':\n",
    "Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance.\n",
    "\n",
    " - 'samples':\n",
    "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "y = [0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n",
    "y_hat = [1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n",
    "\n",
    "jaccard_score(y_true=y, y_pred=y_hat, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-1 score\n",
    "- TP (True Positive)\n",
    "    - is when we predict it as its True and it is actually True  \n",
    "- TN (True Negative):\n",
    "    - is when we predict it as its False and it is actually False  \n",
    "- FP (False Positive):\n",
    "    - is when we predict it as its True __but__ its is actually False\n",
    "- FN (False Negative):\n",
    "    - is when we predict it as its False __but__ its is actually True  \n",
    "\n",
    "<small>\n",
    "    sometimes one of them are more important from another for example maybe we don't care if we do some preventions for something that will not happened(FP),\n",
    "    but we don't want to something unexpected happened (FN)\n",
    "</small>\n",
    "\n",
    "- Precision = TP / (TP + FP)  \n",
    "- Recall = TP / (TP + FN)  \n",
    "- F1-score = $ 2 \\times \\frac {Precision + Recall}{Precision + Recall} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y = [0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n",
    "y_hat = [1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n",
    "\n",
    "f1_score(y , y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Loss\n",
    "\n",
    "since we are getting a categorical value for the labels,\n",
    "they may not be accurate for example:\n",
    "imagine we put person1, person2 in group 1, but we were so certain about person one but not so certain about person2 \n",
    "we can calculate this error with log-loss\n",
    "\n",
    "LogLoss = $ - \\frac{1}{n} \\sum(y \\times \\log(\\hat{y}) + (1 - y) \\times log(1 - \\hat{y})) $  \n",
    "  \n",
    "$ 0 \\le $ LogLoss $ \\le 1  $\n",
    "\n",
    "less Log-loss means more accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2372206642057339"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "y_true = [0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n",
    "y_pred = [0.3, 0.2, 0.1, 0.8, 0.9, 0.4, 0.7, 0.9, 0.2, 0.85] \n",
    "\n",
    "log_loss(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
